
# Hybrid RL Loop Specification for ULS (`rl_loop_spec.md`)

> This document defines the hybrid reinforcement learning loop used by agents in SAAF‚ÄëOS, combining direct policy optimization with Monte Carlo Tree Search (MCTS) in the Unified Latent Space (ULS).

---

## üéØ Goals

- Use contradiction gradients and latent flows for plan evaluation and improvement.
- Allow both reactive behavior and deliberative planning.
- Integrate with Forward World Model (FWM), Contradiction Engine, and Governance.

---

## üß© Loop Architecture

```mermaid
flowchart TD
    Start([Receive z_t, u_t])
    Policy[Policy œÄ_Œ∏(z_t) ‚Üí a_t]
    FWM[Forward World Model simulates rollouts]
    MCTS[MCTS in Latent Space using FWM]
    Rank[Score using contradiction loss & value vector]
    Plan[Select œÄ*(z_t) or refine œÄ_Œ∏]
    Execute[Send action to Actuator Module]
    Learn[Backprop Œ∏ using plan feedback]

    Start --> Policy
    Policy --> MCTS
    MCTS --> Rank
    Rank --> Plan
    Plan --> Execute
    Execute --> Learn
    Learn --> Policy
```

---

## üîπ Components

### 1. Direct Policy Network

- **Input**: `z_t ‚àà ùìõ` (ULS state)
- **Output**: Action `a_t`
- **Training**: RL algorithm (e.g. PPO, A2C, DDPG)
- **Reward**:
  - `R_t = -L_contradiction(z_t) + ‚àë_{i=1}^4 w_i * ValueVector_i`

---

### 2. Monte Carlo Tree Search in Latent Space

- **Simulator**: Forward World Model (`F(z_t, a_t)`)
- **Nodes**: Latent states `z_t'`
- **Scoring**:
  - Primary: `-L_contradiction(z_t')`
  - Secondary: Value vector decomposition (labor_time, surplus_value, etc.)
- **Bias**:
  - Use `‚àág L_contradiction(z_t)` to guide expansion
- **Termination**:
  - Max depth or contradiction convergence

---

### 3. Plan Selection Logic

- **Training Mode**:
  - Use MCTS plans to supervise policy learning (imitation loss or RLHF)
- **Runtime Mode**:
  - Use MCTS only when:
    - High contradiction score
    - High epistemic uncertainty
    - Policy confidence below threshold

---

## üîß Implementation Notes

- FWM must support fast batched rollouts (`simulate()` API)
- Contradiction Engine must expose scoring function (`L_contradiction`)
- Governance hook triggers vote if proposed plan exceeds policy impact threshold
- All actions logged to episodic memory with `z_t`, `a_t`, `L_contradiction(z_t)` for future recall

---

## üß† Advanced Features

### Plan Caching
- Store top-K plans per class/goal type for reuse
- Hash by `(z_t, goal, class_vector)`

### Latent Perturbation Search
- Sample `z_t' ~ N(z_t, œÉ¬≤I)` and run rollout ensembles
- Select plan minimizing contradiction + energy + InfoNCE loss

### Policy Distillation
- After MCTS runs, use best plan trace to fine-tune `œÄ_Œ∏`
- Encode plan trajectory as sequence-to-sequence imitation target

---

## ‚úÖ Summary

This hybrid RL loop leverages the structured planning capabilities of MCTS in latent space while enabling scalable, fast-reactive behavior through policy optimization. It integrates dialectical contradiction scoring, multi-signal loss decomposition, and polycentric governance‚Äîall aligned with ULS theory.

